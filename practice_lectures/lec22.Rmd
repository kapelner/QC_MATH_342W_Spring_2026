---
title: "Practice Lecture 22 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---



# Missingness

Take a look at an housing dataset from Australia:

https://www.kaggle.com/dansbecker/melbourne-housing-snapshot/home?select=melb_data.csv#


```{r}
rm(list = ls())
pacman::p_load(tidyverse, magrittr, data.table, skimr, R.utils)
apts = fread("melb_data.csv.bz2")
skim(apts)
```

We drop all character variables first just for expedience in the demo. If you were building a prediction model, you would scour them carefully to see if there is any signal in them you can use, and then mathematize them to metrics if so.

```{r}
apts %<>%
  select_if(is.numeric) %>%
  select(Price, everything())
```

Imagine we were trying to predict `Price`. So let's section our dataset:

```{r}
y = apts$Price
X = apts %>% 
  select(-Price)
rm(apts)
```

Let's first create a matrix with $p$ columns that represents missingness

```{r}
M = as_tibble(apply(is.na(X), 2, as.numeric))
colnames(M) = paste("is_missing_", colnames(X), sep = "")
M %<>% 
  select_if(function(x){sum(x) > 0})
head(M)
skim(M)
```

Some of these missing indicators might be collinear because they share all the rows they are missing on. Let's filter those out if they exist:

```{r}
M = as_tibble(t(unique(t(M))))
skim(M)
```

Without imputing and without using missingness as a predictor in its own right, let's see what we get with a basic linear model now:

```{r}
lin_mod_listwise_deletion = lm(y ~ ., X)
summary(lin_mod_listwise_deletion)
```

Not bad ... but this is only on the data that has full records! There are 6,750 observations dropped!

Now let's impute using the package. we cannot fit RF models to the entire dataset (it's 13,580 observations) so we will sample 2,000 observations for each of the trees. This is a typical strategy when fitting RF. It definitely reduces variance but increases bias. But we don't have a choice since we don't want to wait forever. We will see that boosting is faster so it is preferred for large sample sizes.

```{r}
pacman::p_load(missForest)
Ximp = missForest(data.frame(X), sampsize = rep(2000, ncol(X)))$ximp
skim(Ximp)
```


Now we consider our imputed dataset as the design matrix.

```{r}
linear_mod_impute = lm(y ~ ., Ximp)
summary(linear_mod_impute)
```
We can do even better if we use all the information i.e. including the missingness. We take our imputed dataset, combine it with our missingness indicators for a new design matrix.

```{r}
Ximp_and_missing_dummies = data.frame(cbind(Ximp, M))
linear_mod_impute_and_missing_dummies = lm(y ~ ., Ximp_and_missing_dummies)
summary(linear_mod_impute_and_missing_dummies)
```

Not much gain, but it the right thing to do. For those in 343... it checks out nicely:

```{r}
anova(linear_mod_impute, linear_mod_impute_and_missing_dummies)
```


Are these two better models than the original model that was built with listwise deletion of observations with missingness?? 

Are they even comparable? It is hard to compare the two models since the first model was built with only non-missing observations which may be easy to predict on and the second was built with the observations that contained missingness. Those extra 6,750 are likely more difficult to predict on. So we cannot do the comparison!

Maybe one apples-to-apples comparison is you can replace all the missingness in the original dataset with something naive e.g. the average and then see who does better. This at least keeps the same observations.

```{r}
X %<>% mutate(Rooms = as.numeric(Rooms))
Xnaive = X %>%
 replace_na(as.list(colMeans(X, na.rm = TRUE)))
linear_mod_naive_without_missing_dummies = lm(y ~ ., Xnaive)
summary(linear_mod_naive_without_missing_dummies)
```

There is a clear gain to imputing and using is_missing dummy features to reduce delta (55.3% vs 52.4% Rsqs).

Note: this is just an illustration of best practice. It didn't necessarily have to "work".


