pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
adult$native_country = NULL
adult$income = ifelse(adult$income == ">50K", 1, 0)
n = 5000
set.seed(3)
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
#train on train set
svm_mod = svm(income ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
ggplot(data.frame(cost = cost_grid, miscl_err = select_set_misclassification_errors_by_m)) +
aes(x = cost, y = miscl_err) +
geom_line(color = "grey") +
geom_point(lwd = 3) +
scale_x_log10()
y_hat
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
adult$native_country = NULL
# adult$income = ifelse(adult$income == ">50K", 1, 0)
n = 5000
set.seed(3)
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
#train on train set
svm_mod = svm(income ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
head(y_hat)
mean(adult_select$income != y_hat)
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
adult$native_country = NULL
adult$income = factor(adult$income)
n = 5000
set.seed(3)
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
ggplot(data.frame(cost = cost_grid, miscl_err = select_set_misclassification_errors_by_m)) +
aes(x = cost, y = miscl_err) +
geom_line(color = "grey") +
geom_point(lwd = 3) +
scale_x_log10()
M = 50
cost_grid = 10^seq(from = -5, to = 3, length.out = M)
cost_grid
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
ggplot(data.frame(cost = cost_grid, miscl_err = select_set_misclassification_errors_by_m)) +
aes(x = cost, y = miscl_err) +
geom_line(color = "grey") +
geom_point(lwd = 3) +
scale_x_log10()
M = 30
cost_grid = 10^seq(from = -5, to = 4, length.out = M)
cost_grid
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
ggplot(data.frame(cost = cost_grid, miscl_err = select_set_misclassification_errors_by_m)) +
aes(x = cost, y = miscl_err) +
geom_line(color = "grey") +
geom_point(lwd = 3) +
scale_x_log10()
M = 30
cost_grid = 10^seq(from = -3, to = 6, length.out = M)
cost_grid
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
ggplot(data.frame(cost = cost_grid, miscl_err = select_set_misclassification_errors_by_m)) +
aes(x = cost, y = miscl_err) +
geom_line(color = "grey") +
geom_point(lwd = 3) +
scale_x_log10()
M = 30
cost_grid = 10^seq(from = -3, to = 5, length.out = M)
cost_grid
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
?svm
?svm.default
svm.default
MAX_NUM_ITER =
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m], scale = TRUE)
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
TOL = 0.01
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m], tolerance = TOL)
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
ggplot(data.frame(cost = cost_grid, miscl_err = select_set_misclassification_errors_by_m)) +
aes(x = cost, y = miscl_err) +
geom_line(color = "grey") +
geom_point(lwd = 3) +
scale_x_log10()
M = 30
cost_grid = 10^seq(from = -3, to = 5, length.out = M)
cost_grid
TOL = 0.01
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m], tolerance = TOL)
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
TOL = 0.1
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m], tolerance = TOL)
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
adult$native_country = NULL
adult$income = factor(adult$income)
n = 3000
set.seed(3)
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
M = 30
cost_grid = 10^seq(from = -3, to = 5, length.out = M)
cost_grid
TOL = 0.1
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m], tolerance = TOL)
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
ggplot(data.frame(cost = cost_grid, miscl_err = select_set_misclassification_errors_by_m)) +
aes(x = cost, y = miscl_err) +
geom_line(color = "grey") +
geom_point(lwd = 3) +
scale_x_log10()
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
adult$native_country = NULL
adult$income = factor(adult$income)
n = 2000
set.seed(3)
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
M = 30
cost_grid = 10^seq(from = -3, to = 4, length.out = M)
cost_grid
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
adult$native_country = NULL
adult$income = factor(adult$income)
n = 2000
set.seed(1)
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
adult$native_country = NULL
adult$income = factor(adult$income)
n = 2000
set.seed(3)
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
adult$native_country = NULL
adult$income = factor(adult$income)
n = 2000
set.seed(4)
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult)
adult$native_country = NULL
adult$income = factor(adult$income)
n = 2000
set.seed(5)
adult = adult[sample(1 : nrow(adult), n), ]
adult_train =  adult[1 : (n / 3), ]
adult_select = adult[(n / 3 + 1) : (2 * n / 3), ]
adult_test =   adult[(2 * n / 3 + 1) : n, ]
select_set_misclassification_errors_by_m = array(NA, M)
for (m in 1 : M){
cat(m, "of", M, "\n")
#train on train set
svm_mod = svm(factor(income) ~ ., adult_train, kernel = "linear", cost = cost_grid[m])
#predict on select set
y_hat = predict(svm_mod, adult_select)
#measure error from the select set
select_set_misclassification_errors_by_m[m] = mean(adult_select$income != y_hat)
}
ggplot(data.frame(cost = cost_grid, miscl_err = select_set_misclassification_errors_by_m)) +
aes(x = cost, y = miscl_err) +
geom_line(color = "grey") +
geom_point(lwd = 3) +
scale_x_log10()
min(select_set_misclassification_errors_by_m)
optimal_cost_hyperparam = cost_grid[which.min(select_set_misclassification_errors_by_m)]
optimal_cost_hyperparam
svm_mod = svm(income ~ ., adult_train, kernel = "linear", cost = optimal_cost_hyperparam)
y_hat = predict(svm_mod, adult_test)
mean(adult_test$income != y_hat)
g_final = svm(income ~ ., adult, kernel = "linear", cost = optimal_cost_hyperparam)
rm(list = ls())
diamonds = ggplot2::diamonds
diamonds$cut = factor(diamonds$cut, ordered = FALSE)
diamonds$color = factor(diamonds$color, ordered = FALSE)
diamonds$clarity = factor(diamonds$clarity, ordered = FALSE)
Nsamp = 1300
set.seed(1984)
subindices = sample(1 : nrow(diamonds), Nsamp * 3)
diamonds_train = diamonds[subindices[1 : Nsamp], ]
diamonds_select = diamonds[subindices[(Nsamp + 1) : (2 * Nsamp)], ]
diamonds_test = diamonds[subindices[(2 * Nsamp + 1) : (3 * Nsamp)], ]
rm(subindices)
mod = lm(price ~ . * . * ., diamonds_train)
length(coef(mod))
sample(names(coef(mod)), 100)
summary(mod)$r.squared
sd(summary(mod)$residuals)
y_hat_test = predict(mod, diamonds_test)
y_test = diamonds_test$price
e_test = y_test - y_hat_test
1 - sum((e_test)^2) / sum((y_test - mean(y_test))^2)
sd(e_test)
sd(y_test)
sd(e_test) / sd(y_test)
Xmm_train = model.matrix(price ~ . * . * ., diamonds_train)
y_train = diamonds_train$price
p_plus_one = ncol(Xmm_train)
Xmm_select = model.matrix(price ~ . * . * ., diamonds_select)
y_select = diamonds_select$price
included_features_by_iter = c() #keep a growing list of predictors by iteration
in_sample_ses_by_iteration = c() #keep a growing list of se's by iteration
oos_ses_by_iteration = c() #keep a growing list of se's by iteration
i = 1
repeat {
#get all predictors left to try
all_ses = array(NA, p_plus_one) #record all possibilities
for (j_try in 1 : p_plus_one){
if (j_try %in% included_features_by_iter){
next
}
Xmm_sub = Xmm_train[, c(included_features_by_iter, j_try), drop = FALSE]
all_ses[j_try] = sd(lm.fit(Xmm_sub, y_train)$residuals) #lm.fit so much faster than lm!
}
j_star = which.min(all_ses)
included_features_by_iter = c(included_features_by_iter, j_star)
in_sample_ses_by_iteration = c(in_sample_ses_by_iteration, all_ses[j_star])
#now let's look at oos
Xmm_sub = Xmm_train[, included_features_by_iter, drop = FALSE]
mod = lm.fit(Xmm_sub, y_train)
y_hat_select = Xmm_select[, included_features_by_iter, drop = FALSE] %*% mod$coefficients
oos_se = sd(y_select - y_hat_select)
oos_ses_by_iteration = c(oos_ses_by_iteration, oos_se)
cat("i =", i, "in sample: se = ", round(all_ses[j_star], 1), "oos_se", round(oos_se, 1), "added:", colnames(Xmm_train)[j_star], "\n")
i = i + 1
if (i > Nsamp || i > p_plus_one){
break #why??
}
}
simulation_results = data.frame(
iteration = 1 : length(in_sample_ses_by_iteration),
in_sample_ses_by_iteration = in_sample_ses_by_iteration,
oos_ses_by_iteration = oos_ses_by_iteration
)
pacman::p_load(latex2exp)
ggplot(simulation_results) +
geom_line(aes(x = iteration, y = in_sample_ses_by_iteration), col = "red") +
geom_line(aes(x = iteration, y = oos_ses_by_iteration), col = "blue") +
ylim(0, max(c(simulation_results$in_sample_ses_by_iteration, simulation_results$oos_ses_by_iteration)))
ylab(TeX("$s_e$"))
p_opt = 41 #change this based on plot above
oos_ses_by_iteration[p_opt]
sort(colnames(Xmm_train)[included_features_by_iter[1 : p_opt]])
Xmm_test = model.matrix(price ~ . * . * ., diamonds_test)
optimal_mod = lm(diamonds_test$price ~ 0 + Xmm_test[, included_features_by_iter[1 : p_opt]])
summary(optimal_mod)$sigma
205.65/4470.91
n = 37
boston_sub = MASS::Boston[1 : n, ]
X = model.matrix(medv ~ ., boston_sub)
n = 37
boston_sub = MASS::Boston[1 : n, ]
X = model.matrix(medv ~ ., boston_sub)
y = boston_sub$medv
solve(t(X) %*% X)
X = model.matrix(medv ~ ., boston_sub)
solve(t(X) %*% X)
n = 20
boston_sub = MASS::Boston[1 : n, ]
X = model.matrix(medv ~ ., boston_sub)
y = boston_sub$medv
solve(t(X) %*% X)
X
X = model.matrix(medv ~ ., MASS::Boston)
y = MASS::Boston$medv
solve(t(X) %*% X)
round(solve(t(X) %*% X), 2)
round(solve(t(X) %*% X), 5)
?round
signif(solve(t(X) %*% X), 3)
signif(solve(t(X) %*% X), 2)
summary(lm(medv ~ ., MASS::boxcox()))
summary(lm(medv ~ ., MASS::Boston)
)
X = model.matrix(medv ~ zn + rm + dis + lstat, MASS::Boston)
y = MASS::Boston$medv
signif(solve(t(X) %*% X), 2)
X = model.matrix(medv ~ zn + rm + nox + dis + lstat, MASS::Boston)
y = MASS::Boston$medv
signif(solve(t(X) %*% X), 2)
1704.77+189
f = medv ~ zn + rm + nox + dis + lstat
summary(lm(f, MASS::Boston))
mean(y)
sd(y)
round(mean(y), 2)
round(mean(y), 2)
round(sd(y), 2)
mod = lm(f, MASS::Boston)
coef(mod)
round(coef(mod), 2)
signif(XtXinv, 2)
XtXinv = solve(t(X) %*% X)
signif(XtXinv, 2)
H = X %*% XtXinv %*% t(X)
signif(H[1:6,1:6], 2)
summary(lm(medv ~ rm + lstat, MASS::Boston))
mod = lm(medv ~ rm + lstat, MASS::Boston)
summary(mod)
mod$model
X = model.matrix(f_red, MASS::Boston)
XtXinv = solve(t(X) %*% X)
XtXinv %*% X %*% diag(mod$residuals^2) %*% X %*% XtXinv
XtXinv = solve(t(X) %*% X)
XtXinv %*% t(X) %*% diag(mod$residuals^2) %*% X %*% XtXinv
signif(XtXinv %*% t(X) %*% diag(mod$residuals^2) %*% X %*% XtXinv, 2)
signif(XtXinv %*% t(X) %*% diag(mod$residuals^2) %*% X %*% XtXinv, 1)
signif(XtXinv %*% t(X) %*% diag(mod$residuals^2) %*% X %*% XtXinv, 2)
round(XtXinv %*% t(X) %*% diag(mod$residuals^2) %*% X %*% XtXinv, 2)
f_red = medv ~ rm + lstat
mod = lm(f_red, MASS::Boston)
summary(mod)
X = model.matrix(f_red, MASS::Boston)
XtXinv = solve(t(X) %*% X)
round(XtXinv %*% t(X) %*% diag(mod$residuals^2) %*% X %*% XtXinv, 2)
??GSSVocab
###problem 3
D = carData
###problem 3
pacman::p_load(carData)
vocab = carData::GSSVocab
###problem 3
pacman::p_load(carData)
vocab = carData::GSSvocab
vocab
?vocab
?carData::GSSvocab
D = carData::GSSvocab
mod = MASS::glm.nb(vocab ~ ., D)
summary(mod)
mod = MASS::glm.nb(vocab ~ . -educGroup, D)
summary(mod)
mod = MASS::glm.nb(vocab ~ . -educGroup - year, D)
summary(mod)
mod = MASS::glm.nb(vocab ~ . -educGroup - year -ageGroup, D)
summary(mod)
exp(-0.0273817)
mod = MASS::glm.nb(vocab ~ . -educGroup - year -ageGroup -gender - nativeBorn, D)
summary(mod)
-115304.3 - -115635.4
