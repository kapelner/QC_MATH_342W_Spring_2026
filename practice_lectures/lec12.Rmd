---
title: "Practice Lecture 12 MATH 342W Queens College"
author: "Professor Adam Kapelner"
---


## Overfitting

Let's see how overfitting increases generalized estimation error.

```{r}
pacman::p_load(ggplot2)
set.seed(1)

p_matters = 10
bbeta = rep(1, p_matters + 1)
sigma_noise = 0.3

#build training data
n = 100
X = cbind(1, matrix(rnorm(n * p_matters), nrow = n))
y = X %*% bbeta + rnorm(n, 0, sigma_noise) #y_vec_past = X_past beta_vec + epsilon_vec_past
SST = sum((y - mean(y))^2)
#\amthbb{D}

#build test data
n_star = 1000
X_star = cbind(1, matrix(rnorm(n_star * p_matters), nrow = n_star))
y_star = X_star %*% bbeta + rnorm(n_star, 0, sigma_noise) #y_vec_future = X_future beta_vec + epsilon_vec_future
SST_star = sum((y_star - mean(y_star))^2)

 #stationarity assumed i.e. 
#beta_vec, process to generate X's, process to generate epsilons same from past to future

all_bs = matrix(NA, n, n)
all_bs[p_matters + 1, 1 : (p_matters + 1)] = coef(lm(y ~ 0 + X))
in_sample_rmse_by_p = array(NA, n)
in_sample_Rsq_by_p = array(NA, n)
for (j in 1 : n){
  if (j > (p_matters + 1)){
    X = cbind(X, rnorm(n))
  }
  lm_mod = lm(y ~ 0 + X[, 1 : j])
  all_bs[j, 1 : j] = coef(lm_mod)
  y_hat = X[, 1 : j] %*% t(all_bs[j, 1 : j, drop = FALSE])
  SSE = sum((y - y_hat)^2)
  in_sample_rmse_by_p[j] = sqrt(SSE / (n - j - 1))
  in_sample_Rsq_by_p[j] = 1 - SSE / SST
}
ggplot(data.frame(num_features = 1 : n, in_sample_rmse_by_p = in_sample_rmse_by_p)) + 
  geom_point(aes(x = num_features, y = in_sample_rmse_by_p))
ggplot(data.frame(num_features = 1 : n, in_sample_Rsq_by_p = in_sample_Rsq_by_p)) + 
  geom_point(aes(x = num_features, y = in_sample_Rsq_by_p))
ggplot(data.frame(num_features = 1 : n, in_sample_Rsq_by_p = in_sample_Rsq_by_p)) + 
  geom_point(aes(x = num_features, y = log10(1 - in_sample_Rsq_by_p)))


bbeta
all_bs[1 : (p_matters + 1), 1 : (p_matters + 1)]
all_bs[91:100, ]
bbeta_matrix = matrix(0, n, n)
bbeta_matrix[, 1 : (p_matters + 1)] = matrix(rep(bbeta, n), nrow = n, byrow = TRUE)
b_error_by_p = apply((all_bs - bbeta_matrix)^2, 1, function(j){sum(j, na.rm = TRUE)})
ggplot(data.frame(num_features = 1 : n, b_error_by_p = b_error_by_p)) + 
  geom_point(aes(x = num_features, y = b_error_by_p))

#look at out of sample error with random features too
oos_rmse_by_p = array(NA, n)
oos_Rsq_by_p = array(NA, n)
X_star = cbind(X_star, matrix(rnorm(n_star * (n - (p_matters + 1))), nrow = n_star))
for (j in 1 : n){
  y_hat_star = X_star[, 1 : j] %*% t(all_bs[j, 1 : j, drop = FALSE])
  oos_SSE = sum((y_star - y_hat_star)^2)
  oos_rmse_by_p[j] = sqrt(oos_SSE / n_star)
  oos_Rsq_by_p[j] = 1 - oos_SSE / SST_star
}
ggplot(data.frame(num_features = 1 : n, oos_rmse_by_p = oos_rmse_by_p)) + 
  geom_point(aes(x = num_features, y = oos_rmse_by_p))
ggplot(data.frame(num_features = 1 : n, oos_Rsq_by_p = oos_Rsq_by_p)) + 
  geom_point(aes(x = num_features, y = oos_Rsq_by_p))
ggplot(data.frame(num_features = 1 : n, oos_Rsq_by_p = oos_Rsq_by_p)) + 
  geom_point(aes(x = num_features, y = oos_Rsq_by_p)) + 
  ylim(0, 1)

#both together
ggplot(data.frame(num_features = 1 : n, in_sample_rmse_by_p = in_sample_rmse_by_p, oos_rmse_by_p = oos_rmse_by_p)) + 
  geom_line(aes(x = num_features, y = oos_rmse_by_p), color = "darkgreen") + 
  geom_line(aes(x = num_features, y = in_sample_rmse_by_p), color = "red")
ggplot(data.frame(num_features = 1 : n, in_sample_Rsq_by_p = in_sample_Rsq_by_p, oos_Rsq_by_p = oos_Rsq_by_p)) + 
  geom_line(aes(x = num_features, y = oos_Rsq_by_p), color = "darkgreen") + 
  geom_line(aes(x = num_features, y = in_sample_Rsq_by_p), color = "red")

```

Let's look at overfitting from another angle. Let's generate a linear model ($f = h^* \in \mathcal{H}$) and let $\epsilon$ be random noise (the error due to ignorance) for $\mathbb{D}$ featuring $n = 2$.

This simulation is random, but to ensure it looks the same to me right now as it does in class (and to you at home), let's "set the seed" so it is deterministic.

```{r}
rm(list = ls())
set.seed(1003)
```

Now let's "randomly generate" the data:

```{r}
n = 2
beta_0 = 1
beta_1 = 1
x = rnorm(n)
#best possible model
h_star_x = beta_0 + beta_1 * x

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon

#scatterplot it
pacman::p_load(ggplot2)
basic = ggplot(data.frame(x = x, y = y, h_star_x = h_star_x), aes(x, y)) +
  geom_point() +
  xlim(-4, 4) + ylim(-5, 5)
basic
```

Let's now fit a linear model to this and plot:

```{r}
mod = lm(y ~ x)
b_0 = coef(mod)[1]
b_1 = coef(mod)[2]
basic + geom_abline(intercept = b_0, slope = b_1, col = "blue")
```

Note that obviously:

```{r}
summary(mod)$r.squared
summary(mod)$sigma #RMSE - technically cannot divide by zero so cannot estimate the RMSE so we use s_e instead
sd(mod$residuals) #s_e
```

And let's plot the true function $h^*$ below as well in green:

```{r}
basic_and_lines = basic + 
  geom_abline(intercept = b_0, slope = b_1, col = "blue") +
  geom_abline(intercept = beta_0, slope = beta_1, col = "darkgreen") + 
  geom_segment(aes(x = x, y = h_star_x, xend = x, yend = y), col = "red")
basic_and_lines
```

The red lines are the epsilons:

```{r}
epsilon
```

Now let's envision some new data not in $\mathbb{D}$. We will call this "out of sample" (oos) since $\mathbb{D}$ defined our "sample" we used to build the model. For the oos data, we predict on it using our linear model $g$ which is far from the best linear model $h^*$ and we look at its residuals $e$:

```{r}
n_new = 50
x_new = rnorm(n_new)
h_star_x_new = beta_0 + beta_1 * x_new
epsilon_new = rnorm(n_new)
y_new = h_star_x_new + epsilon_new
y_hat_new = b_0 + b_1 * x_new

df_new = data.frame(x = x_new, y = y_new, h_star_x = h_star_x_new, y_hat = y_hat_new, e = y - y_hat_new)

basic_and_lines + 
  geom_point(data = df_new) + 
  geom_segment(data = df_new, aes(x = x, y = y_hat_new, xend = x, yend = y), col = "purple")
```

How did we get in this mess? We can see from the picture we are using the blue line as $g$ but we should be using the green line $h^*$. We are using the blue line because we used the original $\epsilon$ to fit. BAD idea - won't generalize to the future.

```{r}
rm(list = ls())
```

The problem is bad for $n = 2$. But is it always bad? Let's take a look at $n = 100$ with a strong linear relationship with one predictor. 

```{r}
set.seed(1003)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
x = runif(n, xmin, xmax)
#best possible model
h_star_x = beta_0 + beta_1 * x

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

The true relationship is plotted below in green.

```{r}
df = data.frame(x = x, y = y, h_star_x = h_star_x)
basic = ggplot(df, aes(x, y)) +
  geom_point() +
  geom_abline(intercept = beta_0, slope = beta_1, col = "darkgreen")
basic
```

And the estimated line $g$ (in blue) is pretty close:

```{r}
mod = lm(y ~ x)
b = coef(mod)
basic +
  geom_abline(intercept = b[1], slope = b[2], col = "blue")
```

They're basically right on top of each other: estimation error near zero.

```{r}
b
c(beta_0, beta_1)
```

And the $R^2$ is:

```{r}
summary(mod)$r.squared
```

Not great... but we can't really do better as here h* = f and the noise is ignorance error. But that's not the point here. The point is that lack of fit gives us plenty of room for overfitting the nonsense epsilons.

Now what happens if we add a random predictor? 

* We know that $R^2$ will go up. 
* But since this predictor is random, it is independent of the best model $h^*(x)$, hence it will constitute *overfitting*. 
* Overfitting is bad because it induces estimation error and $g$ will diverge from $h*$ (previous demo)
* This divergence leads to bad oos error (generalization error) meaning subpar predictions when we actually use the model in the future

Okay - but how bad? It only depends on how much $g$ diverges from $h^*$. Let's look at this divergence slowly. We create a new oos data set made from evenly spaced $x$ values across its range and random values of the nonsense predictors. We use this to calculated oos $s_e$.

```{r}
#n = 100 so p_fake cannot be more than 98
p_fake = 90
set.seed(1984)
X = data.frame(matrix(c(x, rnorm(n * p_fake)), ncol = 1 + p_fake, nrow = n))
mod = lm(y ~ ., X)
t(t(coef(mod)))
y_hat = predict(mod, X)
s_e_in_sample = sd(y - y_hat)

nstar = 1000
set.seed(1984)
Xstar = matrix(c(seq(xmin, xmax, length.out = nstar), rnorm(nstar * p_fake)), ncol = 1 + p_fake)
y_stars = beta_0 + beta_1 * Xstar[, 1] + rnorm(nstar)
y_hat_stars = cbind(1, Xstar) %*% as.matrix(coef(mod))
s_e_oos = sd(y_stars - y_hat_stars)
basic_with_yhat = basic +
  geom_point(data = data.frame(x = x, y = y_hat), aes(x = x, y = y), col = "purple") + 
  xlim(0, 1) + ylim(-3, 8) +
  ggtitle(
    paste("Linear Model with", p_fake, "fake predictors"), 
    paste("Rsq =", round(summary(mod)$r.squared * 100, 2), "percent, in-sample s_e =", round(s_e_in_sample, 2), "and oos s_e =", round(s_e_oos, 2)))
basic_with_yhat

basic_with_yhat +
  geom_line(data = data.frame(x = Xstar[, 1], y = y_hat_stars), aes(x = x, y = y), col = "orange") + 
  scale_y_continuous()
```

Lesson: it takes a bit of time to overfit badly. Don't worry about a few extra degrees of freedom if you have $n$ much larger than $p$. But it will eventually be corrosive!


# Assessing overfitting in practice

Let's examine this again. This time we use one data set which is split between training and testing.

```{r}
set.seed(1003)
n = 100
beta_0 = 1
beta_1 = 5
xmin = 0
xmax = 1
p = 50
X = matrix(runif(n * p, xmin, xmax), ncol = p)

#best possible model - only one predictor matters!
h_star_x = beta_0 + beta_1 * X[,1 ]

#actual data differs due to information we don't have
epsilon = rnorm(n)
y = h_star_x + epsilon
```

Now we split $\mathbb{D}$ into training and testing. We define $K$ first, the inverse proportion of the test size.

```{r}
K = 5 #i.e. the test set is 1/5th of the entire historical dataset

#a simple algorithm to do this is to sample indices directly
test_indices = sample(1 : n, 1 / K * n)
train_indices = setdiff(1 : n, test_indices)

#now pull out the matrices and vectors based on the indices
X_train = X[train_indices, ]
y_train = y[train_indices]
X_test = X[test_indices, ]
y_test = y[test_indices]

#let's ensure these are all correct
dim(X_train)
dim(X_test)
length(y_train)
length(y_test)
```

Now let's fit the model $g$ to the training data and compute in-sample error metrics:

```{r}
mod = lm(y_train ~ ., data.frame(X_train))
summary(mod)$r.squared
sd(mod$residuals)
```

Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:

```{r}
y_hat_oos = predict(mod, data.frame(X_test))
oos_residuals = y_test - y_hat_oos
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2)
sd(oos_residuals)
```



MUCH worse!! Why? We overfit big time...

Can we go back now and fit a new model and see how we did? NO...

So how are we supposed to fix a "bad" model? We can't unless we do something smarter. We'll get there.