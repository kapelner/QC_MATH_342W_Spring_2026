---
title: "Lab 7"
author: "Your Name Here"
output: pdf_document
---

Let's load up the diamonds dataset and compute ln_price (not raw price) and consider it as the prediction target. 

```{r}
pacman::p_load(ggplot2) #this loads the diamonds data set too
diamonds = ggplot2::diamonds
?diamonds
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)      #convert to nominal
diamonds$color =    factor(diamonds$color, ordered = FALSE)    #convert to nominal
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)  #convert to nominal
diamonds$ln_carat = log(diamonds$carat)
diamonds$ln_price = log(diamonds$price)
```

Create model (A) of ln_price ~ ln_carat.

```{r}
#Model A
mod_a = lm(ln_price ~ ln_carat, diamonds)
```

Create a model (B) of ln_price on ln_carat interacted with clarity and compare its performance with the model (A).

```{r}
#Model B
#TO-DO
```

Which model does better? Why?

#TO-DO

Create a model of (C) ln_price on ln_carat interacted with every categorical feature (clarity, cut and color) and compare its performance with model (B)

```{r}
#Model C
#TO-DO
```

Which model does better? Why?

#TO-DO

Create a model (D) of ln_price on every continuous feature (logs of carat, x, y, z, depth, table) interacted with every categorical feature (clarity, cut and color) and compare its performance with model (C).

```{r}
#Model D
#TO-DO
```

Which model does better? Why?

#TO-DO

What is the p of this model D? Compute with code.

```{r}
#TO-DO
```

Create model (E) which is the same as before except create include the raw features interacted with the categorical features and gauge the performance against (D).

```{r}
#Model E
#TO-DO
```

Which model does better? Why?

#TO-DO

Create model (F) which is the same as before except also include also third degree polynomials of the continuous features interacted with the categorical features and gauge performance against (E). By this time you're getting good with R's formula syntax!

```{r}
#Model F
#TO-DO
```

Which model does better? Why?

#TO-DO

Can you think of any other way to expand the candidate set curlyH? Discuss.

#TO-DO

We should probably assess oos performance now. Sample 2,000 diamonds and use these to create a training set of 1,800 random diamonds and a test set of 200 random diamonds. Define K and do this splitting:

```{r}
#TO-DO
```

Compute in and out of sample performance for models A-F. Use s_e as the metric (standard error of the residuals). Create a list with keys A, B, ..., F to store these metrics. Remember the performances here will be worse than before since before you're using nearly 52,000 diamonds to build a model and now it's only 1,800! 

```{r}
#TO-DO
```

You computed oos metrics only on n_* = 200 diamonds. What problem(s) do you expect in these oos metrics?

#TO-DO

To do the K-fold cross validation we need to get the splits right and crossing is hard. We've developed code for this already in a previous lab.

```{r}
temp = rnorm(n)
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
rm(temp)
head(folds_vec, 200)
```

Do the K-fold cross validation for model F and compute the overall s_e and s_s_e. 

```{r}
#TO-DO
```

Does K-fold CV help reduce variance in the oos s_e? Discuss.

#TO-DO

Imagine using the entire rest of the dataset besides the 2,000 training observations divvied up into slices of 200. Measure the oos error for each slice on Model F in a vector `s_e_s_F` and compute the `s_s_e_F` and also plot it.

```{r}
#TO-DO
ggplot(data.frame(s_e_s_F = s_e_s_F)) + geom_histogram(aes(x = s_e_s_F))
```


#Model Selection with Three Splits: Select from M models

We employ the diamonds dataset and specify M models nested from simple to more complex. We store the models as strings in a list (i.e. a hashset)

```{r}
?ggplot2::diamonds
model_formulas = c(
  "carat",
  "carat + cut",
  "carat + cut + color",
  "carat + cut + color + clarity",
  "carat + cut + color + clarity + x + y + z",
  "carat + cut + color + clarity + x + y + z + depth",
  "carat + cut + color + clarity + x + y + z + depth + table",
  "carat * (cut + color + clarity) + x + y + z + depth + table",
  "(carat + x + y + z) * (cut + color + clarity) + depth + table",
  "(carat + x + y + z + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + x + y + z + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table)) * (cut + color + clarity + poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table))"
)
model_formulas = paste0("price ~ ", model_formulas)
M = length(model_formulas)
```

In order to use the formulas with logs we need to eliminate rows with zeros in those measurements:

```{r}
diamonds_cleaned = ggplot2::diamonds
diamonds_cleaned = diamonds_cleaned[
  diamonds_cleaned$carat > 0 &
  diamonds_cleaned$x > 0 &
  diamonds_cleaned$y > 0 &
  diamonds_cleaned$z > 0 &
  diamonds_cleaned$depth > 0 &
  diamonds_cleaned$table > 0, #all columns
]
```

Split the data into train, select and test. Each set should have 1/3 of the total data.

```{r}
n = nrow(diamonds_cleaned)
set.seed(1)
train_idx = sample(1 : n, round(n / 3))
select_idx = sample(setdiff(1 : n, train_idx), round(n / 3))
test_idx = setdiff(1 : n, c(train_idx, select_idx))
diamonds_train =  diamonds_cleaned[train_idx, ]
diamonds_select = diamonds_cleaned[select_idx, ]
diamonds_test =   diamonds_cleaned[test_idx, ]
```

Find the oosRMSE on the select set for each model. Save the number of df in each model while you're doing this as we'll need it for later.

```{r}
#TO-DO
```

Plot the oosRMSE by model complexity (df in model)

```{r}
pacman::p_load(ggplot2)
#TO-DO
```

Select the best model by oosRMSE and find its oosRMSE on the test set.

```{r}
#TO-DO
```

Did we overfit the select set? Discuss why or why not.

#TO-DO

Create the final model object `g_final`.

```{r}
#TO-DO
```


#Model Selection with Three Splits: Hyperparameter selection

We will use an algorithm that I historically taught in 324W but now moved to 343 so I can teach it more deeply using the Bayesian topics from 341. The regression algorithm is called "ridge" and it involves solving for the slope vector via:

b_ridge := (X^T X + lambda I_(p+1))^-1 X^T y

Note how if lambda = 0, this is the same algorithm as OLS. If lambda becomes very large then b_ridge is pushed towards all zeroes. So ridge is good at weighting only features that matter.

However, lambda is a hyperparameter >= 0 that needs to be selected.

We will work with the boston housing dataset except we will add 250 garbage features consisting of iid N(0,1) realizations. We will also standardize the columns so they're all xbar = 0 and s_x = 1. This is shown to be important in 343.

```{r}
rm(list = ls())
?MASS::Boston
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
n = nrow(X)
p_garbage = 250
set.seed(1)
X = cbind(X, matrix(rnorm(n * p_garbage), nrow = n))
X = apply(X, 2, function(x_dot_j){
                  (x_dot_j - mean(x_dot_j)) / sd(x_dot_j)
                })
X[, 1] = 1 #we standardized the intercept column which became zeroes - make it an intercept again
dim(X)
```


Now we split it into 300 train, 100 select and 106 test. 

```{r}
set.seed(1)
train_idx = sample(1 : n, 300)
select_idx = sample(setdiff(1 : n, train_idx), 100)
test_idx = setdiff(1 : n, c(train_idx, select_idx))
#TO-DO
```

We now create a grid of M = 200 models indexed by lambda. The lowest lambda should be zero (which is OLS) and the highest lambda can be 100.

```{r}
M = 200
lambda_grid = seq(from = 0, to = 100, length.out = M)
```

Now find the oosRMSE on the select set on all models each with their own lambda value.

```{r}
#TO-DO
```

Plot the oosRMSE by the value of lambda.

```{r}
#TO-DO
```

Select the model with the best oosRMSE on the select set and find its oosRMSE on the test set.

```{r}
#TO-DO
```

Create the final model object `g_final`.

```{r}
#TO-DO
```


#Model Selection with Three Splits: Forward stepwise modeling

We will use the adult data

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult) #remove any observations with missingness
n = nrow(adult)
?adult
#let's remove "education" as its duplicative with education_num
adult$education = NULL
```


To implement forward stepwise, we need a "full model" that contains anything and everything we can possible want to use as transformed predictors. Let's first create log features of all the numeric features. Instead of pure log, use log(value + 1) to handle possible zeroes.

```{r}
skimr::skim(adult)
#this gives us the list of numeric features to create logs
adult$log_age = log(adult$age + 1)
adult$log_fnlwgt = log(adult$fnlwgt + 1)
adult$log_education_num = log(adult$education_num + 1)
adult$log_capital_gain = log(adult$capital_gain + 1)
adult$log_capital_loss = log(adult$capital_loss + 1)
adult$log_hours_per_week = log(adult$hours_per_week + 1)
```

Now let's create a model matrix Xfull that contains all first order interactions. How many degrees of freedom in this "full model"?

```{r}
#TO-DO
```

Now let's split it into train, select and test sets. Because this will be a glm, model-building (training) will be slow, so let's keep the training set small at 2,000. Since prediction is fast, we can divide the others evenly among select and test.

```{r}
y = ifelse(adult$income == ">50K", 1, 0)
#TO-DO
Xfull_train =  Xfull[train_idx, ]
Xfull_select = Xfull[select_ids, ]
Xfull_test =   Xfull[test_idx, ]
y_train =      y[train_idx]
y_select =     y[select_idx]
y_test =       y[test_idx]
```

Now let's use the code from class to run the forward stepwise modeling. As this is binary classification, let's use logistic regression and to measure model performance, let's use the Brier score. Compute the Brier score in-sample (on training set) and oos (on selection set) for every iteration of j, the number of features selected from the greedy selection procedure.

```{r}
#TO-DO
```

Plot the in-sample Brier score (in red) and oos Brier score (in blue) by the number of features used.

```{r}
#TO-DO
```

Select the model with the best oos Brier score on the select set and find its oos Brier score on the test set.

```{r}
#TO-DO
```

Create the final model object `g_final`.

```{r}
#TO-DO
```
